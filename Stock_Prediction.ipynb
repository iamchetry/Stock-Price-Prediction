{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stock_Prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNIfCV3PveC+g4MaWCGCa1s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamchetry/Stock-Price-Prediction/blob/main/Stock_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1U9xSkQpge_d",
        "outputId": "aaa47ce5-de86-4713-f162-2ccf42c0d440"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install elephas\n",
        "\n",
        "import pandas as pd\n",
        "from matplotlib.pyplot import *\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession, SQLContext\n",
        "from pyspark.ml.feature import StandardScaler, VectorAssembler, QuantileDiscretizer\n",
        "from pyspark.sql.functions import rand, lead, mean, stddev, col, udf, lit\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, LSTM\n",
        "from tensorflow.keras import optimizers, regularizers\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
        "\n",
        "from elephas.ml_model import ElephasEstimator\n",
        "from google.cloud import bigquery\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/content/drive/My Drive/stock-data-analysis-cse560-b3fc65360aa2.json'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.7/dist-packages (0.1.63)\n",
            "Requirement already satisfied: lxml>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from yfinance) (4.6.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.9)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.7/dist-packages (from yfinance) (2.23.0)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->yfinance) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->yfinance) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24->yfinance) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (3.0.4)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.1.2)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qm8xlmn6hIHj"
      },
      "source": [
        "# Spark Session\n",
        "conf = SparkConf().setAppName('Stock Price Prediction').setMaster('local[2]')\n",
        "sc = SparkContext(conf=conf)\n",
        "sql_context = SQLContext(sc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pM5WgSuFUben"
      },
      "source": [
        "# Connect to BigQuery\n",
        "bqclient = bigquery.Client()\n",
        "\n",
        "def fetch_from_bquery(name=None):\n",
        "  # Download query results.\n",
        "  query_string = ''' select * from stock-data-analysis-cse560.StockInfoDatabase.stock_history where Symbol={};'''.format(name)\n",
        "\n",
        "  df_hist = (\n",
        "      bqclient.query(query_string)\n",
        "      .result()\n",
        "      .to_dataframe(\n",
        "          # Optionally, explicitly request to use the BigQuery Storage API. As of\n",
        "          # google-cloud-bigquery version 1.26.0 and above, the BigQuery Storage\n",
        "          # API is used by default.\n",
        "          create_bqstorage_client=True,\n",
        "      )\n",
        "  )\n",
        "\n",
        "  df_hist.to_csv('df_stock_hist.csv', index=False)\n",
        "  !mv df_stock_hist.csv '/content/drive/My Drive/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTMK-KQBqqug",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "247d3ca0-ecd1-4268-fe74-5301ff41dca0"
      },
      "source": [
        "# Load Spark df\n",
        "fetch_from_bquery(name='GOOGL')\n",
        "df_hist = sql_context.read.csv('/content/drive/My Drive/df_stock_hist.csv', header=True, inferSchema=True)[['date_', 'Open']]\n",
        "df_hist.orderBy('date_', ascending=True, inplace=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[date_: string, Open: double]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BseP-OJ5C5uF"
      },
      "source": [
        "# Creating Lagged Features\n",
        "for _ in range(1, 61):\n",
        "  df_hist = df_hist.withColumn('Open_{}'.format(_), lead('Open', _).over(Window.orderBy('date_')))\n",
        "\n",
        "df_hist = df_hist.drop('date_')\n",
        "df_hist = df_hist.withColumnRenamed('Open_60', 'target_price').withColumnRenamed('Open', 'Open_0')\n",
        "df_hist = df_hist.na.drop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvarrYtUsLe6"
      },
      "source": [
        "# Train Test Split\n",
        "discretizer = QuantileDiscretizer(numBuckets=10, inputCol='target_price', outputCol='bins')\n",
        "\n",
        "df_hist = discretizer.fit(df_hist).transform(df_hist)\n",
        "df_hist.bins = df_hist.bins.astype('int')\n",
        "\n",
        "train = df_hist.sampleBy('bins', fractions={0: 0.8, 1: 0.8, 2: 0.8, 3: 0.8, 4: 0.8, 5: 0.8, 6: 0.8, 7: 0.8, 8: 0.8, 9: 0.8}, seed=10)\n",
        "test = df_hist.subtract(train)\n",
        "\n",
        "df_hist = df_hist.drop('bins')\n",
        "train = train.drop('bins')\n",
        "test = test.drop('bins')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npmqnNKUst94"
      },
      "source": [
        "# Calculate Mean and STD for each column in Train data\n",
        "mean_list = list()\n",
        "std_list = list()\n",
        "\n",
        "for col_ in train.columns:\n",
        "  df_stats = train.select(mean(col(col_)).alias('avg_{}'.format(col_)), stddev(col(col_)).alias('std_{}'.format(col_))).collect()\n",
        "  mean_list.append(df_stats[0]['avg_{}'.format(col_)])\n",
        "  std_list.append(df_stats[0]['std_{}'.format(col_)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZfF3elLkkOf"
      },
      "source": [
        "# Scale Data\n",
        "def z_score(x, mean_, std_):\n",
        "  return (x - mean_)/std_\n",
        "\n",
        "scale_ = udf(lambda x, mean_, std_: z_score(x, mean_, std_), DoubleType())\n",
        "\n",
        "for _, col_ in enumerate(list(df_hist.columns)):\n",
        "  mean_ = mean_list[_]\n",
        "  std_ = std_list[_]\n",
        "\n",
        "  train = train.withColumn(col_+'_scaled', scale_(df_hist[col_], lit(mean_), lit(std_)))\n",
        "  test = test.withColumn(col_+'_scaled', scale_(df_hist[col_], lit(mean_), lit(std_)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQBMdLmnlBrp"
      },
      "source": [
        "# Create Feature Vector and Target Variable\n",
        "assembler = VectorAssembler(inputCols=['Open_{}_scaled'.format(_) for _ in range(60)], outputCol='features')\n",
        "\n",
        "train = assembler.transform(train).select(['features', 'target_price_scaled'])\n",
        "test = assembler.transform(test).select(['features', 'target_price_scaled'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHPOV0LUrzt9"
      },
      "source": [
        "# LSTM Structure\n",
        "input_dim = len(train.select(\"features\").first()[0])\n",
        "model = Sequential()\n",
        "\n",
        "model.add(LSTM(units = 128, return_sequences = True, input_shape = (input_dim, 1),\n",
        "               activity_regularizer=regularizers.l2(0.25)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(LSTM(units = 128, return_sequences = True,\n",
        "               activity_regularizer=regularizers.l2(0.25)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(LSTM(units = 128, return_sequences = True,\n",
        "               activity_regularizer=regularizers.l2(0.25)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(LSTM(units = 128, activity_regularizer=regularizers.l2(0.25)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(units = 1))\n",
        "\n",
        "model.compile(optimizer='adam', loss = 'mean_squared_error')\n",
        "cb = [EarlyStoppingByLossVal(monitor='val_loss', value=0.03, verbose=2)]\n",
        "model.fit(x_train, y_train, epochs = 1000, validation_split=0.2, batch_size = 128, verbose=2, callbacks=cb)\n",
        "\n",
        "sgd = optimizers.SGD(learning_rate=0.001, momentum=0.9)\n",
        "sgd_conf = optimizers.serialize(sgd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZLovinFK1Sz",
        "outputId": "04961e54-a989-4a6f-b3fa-bc170b0d769a"
      },
      "source": [
        "# Initialize Elephas Spark ML Estimator\n",
        "estimator = ElephasEstimator()\n",
        "\n",
        "estimator.set_keras_model_config(model.to_yaml())\n",
        "estimator.set_optimizer_config(sgd_conf)\n",
        "estimator.set_mode(\"synchronous\")\n",
        "estimator.set_loss(\"mae\")\n",
        "estimator.set_metrics(['mse'])\n",
        "estimator.set_epochs(1000)\n",
        "estimator.set_batch_size(128)\n",
        "estimator.set_validation_split(0.2)\n",
        "estimator.set_categorical_labels(False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ElephasEstimator_9e2722c71094"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcPk3WzwK7-E"
      },
      "source": [
        "# Fitting a model returns a Transformer\n",
        "pipeline = Pipeline(stages=[estimator])\n",
        "fitted_pipeline = pipeline.fit(train)\n",
        "\n",
        "# Evaluate Spark model by evaluating the underlying model\n",
        "prediction = fitted_pipeline.transform(test)\n",
        "pnl = prediction.select(\"label\", \"prediction\")\n",
        "\n",
        "prediction_and_label = pnl.rdd.map(lambda row: (row.label, row.prediction))\n",
        "metrics = RegressionMetrics(prediction_and_label)\n",
        "print(metrics.r2)\n",
        "print(metrics.meanAbsoluteError)\n",
        "print(metrics.rootMeanSquaredError)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WBeML5WaeGg"
      },
      "source": [
        "# Evaluate Spark model\n",
        "prediction = fitted_pipeline.transform(train)\n",
        "pnl = prediction.select(\"index_category\", \"prediction\")\n",
        "pnl.show(100)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}